---
layout: post
title: YOLO9000（转）
category: 技术
tags: [Objective]
description: 
---

> 时隔一年，YOLO从v1版本进化到了v2版本，在YOLO的基础上做了比较大的改进，在保持原有速度的同时提升精度得到YOLOv2，可以匹敌Faster-RCNN和SSD了。
原博客在[这里](https://www.jianshu.com/p/2d88bdd89ba0)。

我们先来看看YOLO9000的实验效果：

![](/assets/img/Objective/YOLO90001.png)

作者还提出了一种目标分类与检测的联合训练方法，同时在COCO和ImageNet数据集中进行训练得到YOLO9000，实现9000多种物体的实时检测。

![](/assets/img/Objective/YOLO90002.png)

# 1.Introduction #

目前的检测数据集（Detection Datasets）有很多限制，分类标签的信息太少，图片的数量小于分类数据集（Classiﬁcation Datasets），而且检测数据集的成本太高，
使其无法当作分类数据集进行使用。而现在的分类数据集却有着大量的图片和十分丰富分类信息。

文章提出了一种新的训练方法–联合训练。这种算法可以把这两种的数据集混合到一起。使用一种分层的观点对物体进行分类，用巨量的分类数据集数据来扩充检测数据集，
从而把两种不同的数据集混合起来。 联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（｀Object Detectors｀ ），
用监测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。

YOLO9000就是使用联合训练算法训练出来的，他拥有9000类的分类信息，这些分类信息学习自ImageNet分类数据集，而物体位置检测则学习自COCO检测数据集。

# 2.Better #

YOLO相比于Faster-RCNN，定位误差比较大。与region proposal-based的方法相比，recall也较低。所以作者旨在在保持分类准确率 的同时提高recall,同时降低localization误差。

目前计算机视觉的趋势是更大更深的网络，更好的性能表现通常依赖于训练更大的网络或者把多种model综合到一起。但是YOLO v2则着力于简化网络。具体的改进见下图：

![](/assets/img/Objective/YOLO90003.png)

**Batch Normalization**

使用Batch Normalization对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化（regularization）的依赖。通过对YOLO的每一个卷积层增加Batch Normalization，
最终使得mAP提高了2%(见上图)，同时还使model正则化。使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。

参考文章：[Batch_Normalization](https://buptldy.github.io/2016/08/18/2016-08-18-Batch_Normalization)

**High Resolution Classiﬁer**

所有state-of-the-art的检测方法基本上都会使用ImageNet进行预训练。从Alexnet开始，大多数的分类器都运行在小于256x256的图片上。
而现在YOLO从224x224增加到了448x448，这就意味着网络需要适应新的输入分辨率。

为了适应新的分辨率，YOLO v2的分类网络以448*448的分辨率先在ImageNet上进行Fine Tune，Fine Tune10个epochs，让网络有时间调整他的滤波器（filters），
好让其能更好的运行在新分辨率上，还需要调优用于检测的Resulting Network。最终通过使用高分辨率，mAP提升了4%。

**Convolutional With Anchor Boxes.**

YOLO使用全连接进行bounding box预测（要把1470*1的全链接层reshape为7730的最终特征），这会丢失较多的空间信息定位不准。
YOLOv2借鉴了Faster R-CNN中的anchor思想： 简单理解为卷积特征图上进行滑窗采样，每个中心预测9种不同大小和比例的建议框。由于都是卷积不需要reshape，
很好的保留的空间信息，最终特征图的每个特征点和原图的每个cell一一对应。

Faster R-CNN的方法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。
作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。

为了引入anchor boxes来预测bounding boxes,具体实现如下：

- 移除最后的池化层确保输出的卷积特征图有更高的分辨率。
- 缩减网络，让图片输入分辨率为416 * 416，目的是让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。因为作者观察到，
大物体通常占据了图像的中间位置，可以只用一个中心的cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测。
- 使用卷积层降采样（factor 为32），使得输入卷积网络的416 * 416图片最终得到13 * 13的卷积特征图。（ (416-32+0)/32+1=13 ）
- 把预测类别的机制从空间位置(cell)中解耦，由anchor box同时预测类别和坐标。因为YOLO是由每个cell来负责预测类别，每个cell对应的2个bounding box 负责预测坐标 。
YOLOv2中，不再让类别的预测与每个cell（空间位置）绑定一起，而是让全部放到anchor box中。下面是特征维度示意图:

![](/assets/img/Objective/YOLO90004.png)

使用Anchor Box会让精确度稍微下降，但用了它能让YOLO能预测出大于一千个框，同时recall达到88%，mAP达到69.2%。

**Dimension Clusters（维度聚类）**

作者在使用anchor boxes的时候遇到了两个问题，第一个是anchor boxes的宽高维度往往是精选的先验框（hand-picked priors），虽说在训练过程中网络也会学习调整boxes的宽高维度，
最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。

和以前的精选boxes维度不同，作者使用了K-means聚类方法训练bounding boxes，可以自动找到更好的boxes宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，
也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），
这样的话，error就和box的尺度无关了。

下面左图： 随着k的增大，IOU也在增大（高召回率），但是复杂度也在增加。所以平衡复杂度和IOU之后，最终得到k值为5。
下面右图：5聚类的中心与手动精选的boxes是完全不同的，扁长的框较少瘦高的框较多。

![](/assets/img/Objective/YOLO90005.png)

当然，作者也做了实验来对比两种策略的优劣，如下图，使用聚类方法，仅仅5种boxes的召回率就和Faster R-CNN（61.0）的9种相当。
说明K-means方法的引入使得生成的boxes更具有代表性，为后面的检测任务提供了便利。

![](/assets/img/Objective/YOLO90006.png)

Direct location prediction

用Anchor Box时遇到的第二个问题就是，Anchor 会让model变得不稳定，尤其是在最开始的几次迭代的时候。大多数不稳定因素产生自预测Box的（x,y）位置的时候。

举个例子，预测值tx=1时，box就会往右边移动一个宽度，tx= -1时，box就会往左边移动一个宽度。

这个公式没有任何限制，无论在什么位置进行预测，任何anchor boxes可以在图像中任意一点。模型随机初始化之后将需要很长一段时间才能稳定预测敏感的物体偏移。
因此作者没有采用这种方法，而是预测相对于grid cell的坐标位置，同时把ground truth限制在0到1之间（利用logistic激活函数约束网络的预测值来达到此限制）。

网络在每一个网格单元中预测出5个Bounding Boxes，每个Bounding Boxes有五个坐标值tx，ty，tw，th，t0，他们的关系见下图。
假设一个网格单元对于图片左上角的偏移量是cx，cy，Bounding Boxes Prior的宽度和高度是pw，ph，那么预测的结果见下图的公式：

![](/assets/img/Objective/YOLO90007.png)

![](/assets/img/Objective/YOLO90008.png)

由于我们约束预测位置，参数更容易学习，这使网络更加稳定。使用Dimension Clusters以及直接预测边界框中心位置将YOLO比使用anchor boxes的版本提高了近5％。

**Fine-Grained Features（细粒度特征）**

YOLO修改后的Feature Map大小为13x13，这个尺寸对检测图片中尺寸大物体来说足够了，同时使用这种细粒度的特征对定位小物体的位置可能也有好处。
Faster F-CNN、SSD都使用不同尺寸的Feature Map来取得不同范围的分辨率，而YOLO采取了不同的方法，YOLO加上了一个Passthrough Layer来取得之前的某个26x26分辨率的层的特征。
这个Passthrough layer能够把高分辨率特征与低分辨率特征联系在一起，联系起来的方法是把相邻的特征堆积在不同的Channel之中，这一方法类似与Resnet的Identity Mapping，
从而把26x26x512变成13x13x2048。YOLO中的检测器位于扩展后（expanded ）的Feature Map的上方，所以他能取得细粒度的特征信息，这提升了YOLO 1%的性能。

**Multi-ScaleTraining**

原始YOLO网络使用固定的448x448的图片作为输入，加入anchor boxes后输入变成416x416，由于网络只用到了卷积层和池化层，就可以进行动态调整（检测任意大小图片）。
为了让YOLOv2对不同尺寸图片的具有鲁棒性，在训练的时候也考虑了这一点。

不同于固定网络输入图片尺寸的方法，每经过10批训练（10 batches）就会随机选择新的图片尺寸。网络使用的降采样参数为32，于是使用32的倍数{320,352，…，608}，
最小的尺寸为320x320，最大的尺寸为608x608。 调整网络到相应维度然后继续进行训练。

这种机制使得网络可以更好地预测不同尺寸的图片，同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。

在低分辨率图片检测中，YOLOv2是检测速度快（计算消耗低），精度较高的检测器。输入为228 * 228的时候，帧率达到90FPS，mAP几乎和Faster R-CNN的水准相同。
使得其更加适用于低性能GPU、高帧率视频和多路视频场景。

在高分辨率图片检测中，YOLOv2达到了先进水平（state-of-the-art），VOC2007 上mAP为78.6%，而且超过实时速度要求。下图是YOLOv2和其他网络在VOC2007上的对比：

![](/assets/img/Objective/YOLO90009.png)

# 3.Faster #

YOLO一向是速度和精度并重，作者为了改善检测速度，也作了一些相关工作。

大多数检测网络有赖于VGG-16作为特征提取部分，VGG-16的确是一个强大而准确的分类网络，但是复杂度有些冗余。224x224的图片进行一次前向传播，其卷积层就需要多达306.9亿次浮点数运算。

YOLOv2使用的是基于Googlenet的定制网络，比VGG-16更快，一次前向传播仅需85.2亿次运算。可是它的精度要略低于VGG-16，单张224x224取前五个预测概率的对比成绩为88%和90%。

**Darknet-19**

YOLOv2使用了一个新的分类网络作为特征提取部分，参考了前人的先进经验，比如类似于VGG，作者使用了较多的3x3卷积核，在每一次池化操作后把通道数翻倍。
借鉴了network in network的思想，网络使用了全局平均池化（global average pooling），把1x1的卷积核置于3x3的卷积核之间，用来压缩特征。
也用了batch normalization（前面介绍过）稳定模型训练。

最终得出的基础模型就是Darknet-19，如下图，其包含19个卷积层、5个最大值池化层（maxpooling layers ），下图展示网络具体结构。
Darknet-19运算次数为55.8亿次，imagenet图片分类top-1准确率72.9%，top-5准确率91.2%。

![](/assets/img/Objective/YOLO900010.png)

**Training for classiﬁcation**

网络训练在 ImageNet 1000类分类数据集，训练了160epochs，使用随机梯度下降，初始学习率为0.1， polynomial rate decay with a power of 4, 
weight decay of 0.0005 and momentum of 0.9 。训练期间使用标准的数据扩大方法：随机裁剪、旋转、变换颜色（hue）、变换饱和度（saturation）， 变换曝光度（exposure shifts）。

在训练时，把整个网络在更大的448x448分辨率上Fine Turnning 10个epoches，初始学习率设置为0.001，这种网络达到达到76.5%top-1精确度，93.3%top-5精确度。

**Training for detection**

网络去掉了最后一个卷积层，而加上了三个3x3卷积层，每个卷积层有1024个Filters，每个卷积层紧接着一个1x1卷积层。 对于VOC数据，
网络预测出每个网格单元预测五个Bounding Boxes，每个Bounding Boxes预测5个坐标和20类，所以一共125个Filters，
增加了Passthough层来获取前面层的细粒度信息，网络训练了160epoches，初始学习率0.001，dividing it by 10 at 60 and 90 epochs，
a weight decay of 0.0005 and momentum of 0.9，数据扩大方法相同，对COCO与VOC数据集的训练对策相同。

# 4.Stronger #

作者提出了一种在分类数据集和检测数据集上联合训练的机制。使用检测数据集的图片去学习检测相关的信息，例如bounding box 坐标预测，
是否包含物体以及属于各个物体的概率。使用仅有类别标签的分类数据集图片去扩展可以检测的种类。

训练过程中把监测数据和分类数据混合在一起。当网络遇到一张属于检测数据集的图片就基于YOLOv2的全部损失函数（包含分类部分和检测部分）做反向传播。
当网络遇到一张属于分类数据集的图片就仅基于分类部分的损失函数做反向传播。

这种方法有一些难点需要解决。检测数据集只有常见物体和抽象标签（不具体），例如 “狗”，“船”。分类数据集拥有广而深的标签范围
（例如ImageNet就有一百多类狗的品种，包括 “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”等. ）。必须按照某种一致的方式来整合两类标签。

大多数分类的方法采用softmax层，考虑所有可能的种类计算最终的概率分布。但是softmax假设类别之间互不包含，但是整合之后的数据是类别是有包含关系的，
例如 “Norfolk terrier” 和 “dog”。 所以整合数据集没法使用这种方式（softmax 模型），

作者最后采用一种不要求互不包含的多标签模型（multi-label model）来整合数据集。这种方法忽略了数据集的结构（例如 COCO数据集的所有类别之间是互不包含的）

**Hierarchical classiﬁcation（层次式分类）**

ImageNet的标签参考WordNet（一种结构化概念及概念之间关系的语言数据库）。例如：

![](/assets/img/Objective/YOLO900011.png)

很多分类数据集采用扁平化的标签。而整合数据集则需要结构化标签。

WordNet是一个有向图结构（而非树结构），因为语言是复杂的（例如“dog”既是“canine”又是“domestic animal”），为了简化问题，
作者从ImageNet的概念中构建了一个层次树结构（hierarchical tree）来代替图结构方案。

创建层次树的步骤是：

- 遍历ImageNet的所有视觉名词
- 对每一个名词，在WordNet上找到从它所在位置到根节点（“physical object”）的路径。 许多同义词集只有一条路径。所以先把这些路径加入层次树结构。
- 然后迭代检查剩下的名词，得到路径，逐个加入到层次树。路径选择办法是：如果一个名词有两条路径到根节点，其中一条需要添加3个边到层次树，另一条仅需添加一条边，那么就选择添加边数少的那条路径。

最终结果是一颗 WordTree （视觉名词组成的层次结构模型）。用WordTree执行分类时，预测每个节点的条件概率。例如： 在“terrier”节点会预测：

![](/assets/img/Objective/YOLO900012.png)

如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算：(分类时假设 图片包含物体：Pr(physical object) = 1.)

![](/assets/img/Objective/YOLO900013.png)

为了验证这种方法作者在WordTree（用1000类别的ImageNet创建）上训练了Darknet-19模型。为了创建WordTree1k作者天添加了很多中间节点，把标签由1000扩展到1369。
训练过程中ground truth标签要顺着向根节点的路径传播：例如 如果一张图片被标记为“Norfolk terrier”它也被标记为“dog” 和“mammal”等。
为了计算条件概率，模型预测了一个包含1369个元素的向量，而且基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。

![](/assets/img/Objective/YOLO900014.png)

使用相同的训练参数，层次式Darknet-19获得71.9%的top-1精度和90.4%top-5精度。尽管添加了369个额外概念，且让网络去预测树形结构，精度只有略微降低。
按照这种方式执行分类有一些好处，当遇到新的或未知物体类别，预测精确度降低的很温和（没有突然巨幅下降）。例如：如果网络看到一张狗的图片，
但是不确定狗的类别，网络预测为狗的置信度依然很高，但是，狗的下位词（“xx狗”）的置信度就比较低。

这个策略野同样可用于检测。不在假设每一张图片都包含物体，取而代之使用YOLOv2的物体预测器（objectness predictor）得到Pr(physical object)的值。
检测器预测一个bounding box和概率树（WordTree）。沿着根节点向下每次都走置信度最高的分支直到达到某个阈值，最终预测物体的类别为最后的节点类别。

**Dataset combination with WordTree**

可以使用WordTree把多个数据集整合在一起。只需要把数据集中的类别映射到树结构中的同义词集合（synsets）。使用WordTree整合ImageNet和COCO的标签如下图：

![](/assets/img/Objective/YOLO900015.png)

**joint classification and detection(联合训练分类和检测)**

使用WordTree整合了数据集之后就可以在数据集（分类-检测数据）上训练联合模型。我们想要训练一个检测类别很大的检测器所以使用COCO检测数据集和全部ImageNet的前9000类创造一个联合数据集。
为了评估我们使用的方法，也从ImageNet detection challenge 中向整合数据集添加一些还没有存在于整合数据集的类别。相应的WordTree有9418个类别。
由于ImageNet是一个非常大的数据集，所以通过oversampling COCO数据集来保持平衡，使ImageNet：COCO = 4：1。

使用上面的数据集训练YOLO9000。采用基本YOLOv2的结构，anchor box数量由5调整为3用以限制输出大小。

当网络遇到一张检测图片就正常反向传播。其中对于分类损失只在当前及其路径以上对应的节点类别上进行反向传播。

当网络遇到一张分类图片仅反向传播分类损失。在该类别对应的所有bounding box中找到一个置信度最高的（作为预测坐标），同样只反向传播该类及其路径以上对应节点的类别损失。
反向传播objectness损失基于如下假设：预测box与ground truth box的重叠度至少0.31IOU。

采用这种联合训练，YOLO9000从COCO检测数据集中学习如何在图片中寻找物体，从ImageNet数据集中学习更广泛的物体分类。

作者在ImageNet detection task上评估YOLO9000。ImageNet detection task和COCO有44个物体类别是相同的。这意味着YOLO9000只从大多数测试数据集中看到过分类数据而非检测数据。
最终整体精度为19.7mAP，在从未见过的156个物体检测数据类别上精度为16.0mAP。这个结果高于DPM，但是YOLO9000是在不同数据集上进行半监督训练。而且YOLO9000可以同时实时检测9000多种其它物体类别。

作者也分析了YOLO9000在ImageNet上的性能，发现可以学习新的动物表现很好，但是学习衣服和设备这类物体则不行。因为从COCO数据集上动物类别那里学习到的物体预测泛化性很好。
但是COCO数据集并没有任何衣服类别的标签数据（只有"人"类别），所以YOLO9000很难对“太阳镜”，“游泳裤”这些类别建模。

# 5.Conclusion #

YOLO v2 代表着目前最先进物体检测的水平，在多种监测数据集中都要快过其他检测系统，并可以在速度与精确度上进行权衡。

YOLO 9000 的网络结构允许实时地检测超过9000种物体分类，这归功于它能同时优化检测与分类功能。使用WordTree来混合来自不同的资源的训练数据，并使用联合优化技术同时在ImageNet和COCO数据集上进行训练，YOLO9000进一步缩小了监测数据集与识别数据集之间的大小代沟。

文章还提出了WordTree，数据集混合训练，多尺寸训练等全新的训练方法。

在以后的工作中，可能会涉足弱监督方法用于图像分割。监督学习对于标记数据的要求很高，未来要考虑弱标记的技术，这将会极大扩充数据集，提升训练量。

谢谢观看，希望对您有所帮助，欢迎指正错误，欢迎一起讨论！！！